# âœ… RAG Chat API - FERTIG!

**Datum:** 2025-11-06
**Status:** âœ… Komplett & Ready to use

---

## ğŸ‰ Was wurde erstellt?

### 1. **RAG Chat API** - `/api/rag/chat`

**Datei:** `app/api/rag/chat/route.ts`

**Funktionen:**
- âœ… Nimmt User-Fragen entgegen
- âœ… Embeddet Frage mit OpenAI
- âœ… Sucht Top 5 relevante Chunks in Pinecone
- âœ… Generiert Antwort mit Claude 3.5 Haiku
- âœ… Gibt Antwort + Quellen zurÃ¼ck
- âœ… UnterstÃ¼tzt User-Context (Betriebsart, GrÃ¶ÃŸe, etc.)
- âœ… UnterstÃ¼tzt Pinecone-Filter
- âœ… Error Handling & Validation
- âœ… Logging & Performance-Tracking

**Verwendete Komponenten:**
- `app/lib/ai/rag.ts` - `performRAGQuery()` (orchestriert alles)
- `app/lib/ai/openai.ts` - Embedding Generation
- `app/lib/vectordb/pinecone.ts` - Vector Search
- `app/lib/ai/anthropic.ts` - Claude Response

### 2. **Test Script** - `test-chat.sh`

**Datei:** `test-chat.sh`

- âœ… Testet 2 Beispiel-Fragen
- âœ… Zeigt formatted JSON output
- âœ… AusfÃ¼hrbar mit `./test-chat.sh`

### 3. **Cleanup**

- âœ… Entfernt: `app/lib/utils/pdf-processor.ts` (nicht mehr benÃ¶tigt)
- âœ… Updated: `app/api/rag/embed/status/route.ts` (kein pdf-processor Import mehr)

---

## ğŸš€ Wie du es nutzt

### Schritt 1: Server starten

```bash
# Kill alte Server
lsof -ti:3000 | xargs kill -9

# Neu starten
npm run dev
```

**Warte bis du siehst:**
```
âœ“ Ready in XXXms
```

### Schritt 2: Chat-API testen

**Option A - Mit Test-Script:**

```bash
./test-chat.sh
```

**Option B - Manuell mit curl:**

```bash
curl -X POST http://localhost:3000/api/rag/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Brauche ich eine UVP fÃ¼r ein Restaurant mit 80mÂ²?",
    "userContext": {
      "businessType": "restaurant",
      "businessSize": "80"
    }
  }' | python3 -m json.tool
```

**Option C - Mit user context:**

```bash
curl -X POST http://localhost:3000/api/rag/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Welche Dokumente brauche ich fÃ¼r meinen Antrag?",
    "userContext": {
      "businessType": "restaurant",
      "businessSize": "120",
      "location": "1010 Wien",
      "outdoorSeating": true
    }
  }' | python3 -m json.tool
```

**Option D - Mit Pinecone Filter:**

```bash
curl -X POST http://localhost:3000/api/rag/chat \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Was steht in der Gewerbeordnung Ã¼ber Hygienevorschriften?",
    "filter": {
      "type": {"$eq": "gewerbeordnung"}
    }
  }' | python3 -m json.tool
```

---

## ğŸ“Š Response Format

```json
{
  "success": true,
  "answer": "FÃ¼r ein Restaurant mit 80mÂ² ist in der Regel keine UmweltvertrÃ¤glichkeitsprÃ¼fung (UVP) erforderlich...",
  "sources": [
    {
      "text": "Relevanter Text aus dem Dokument...",
      "source": "gewerberechtl-genehmigungsverfahren.pdf",
      "type": "genehmigungsverordnung",
      "page": 5,
      "section": "Â§ 74",
      "relevanceScore": 0.89
    },
    ...
  ],
  "metadata": {
    "model": "claude-3-5-haiku-20241022",
    "usage": {
      "input_tokens": 2450,
      "output_tokens": 387
    },
    "duration_ms": 3420,
    "timestamp": "2025-11-06T..."
  }
}
```

---

## ğŸ¯ API Endpoint Details

### **POST /api/rag/chat**

**Request Body:**

```typescript
{
  query: string;                 // Required, max 1000 chars
  userContext?: {                // Optional
    businessType?: string;       // e.g., "restaurant", "cafe"
    businessSize?: string;       // e.g., "80" (mÂ²)
    location?: string;           // e.g., "1010 Wien"
    numberOfEmployees?: number;
    outdoorSeating?: boolean;
  };
  filter?: Record<string, any>; // Optional Pinecone filter
}
```

**Response:**

```typescript
{
  success: boolean;
  answer: string;                // Claude's answer
  sources: Array<{
    text: string;                // Chunk text
    source: string;              // PDF filename
    type: string;                // Document type
    page?: number;               // Page number
    section?: string;            // Â§ paragraph
    relevanceScore: number;      // 0-1
  }>;
  metadata: {
    model: string;
    usage: {
      input_tokens: number;
      output_tokens: number;
    };
    duration_ms: number;
    timestamp: string;
  };
}
```

**Error Response:**

```typescript
{
  success: false;
  error: string;
  details?: string;
  duration_ms?: number;
  timestamp: string;
}
```

---

## ğŸ“– API Documentation Endpoint

### **GET /api/rag/chat**

Zeigt API-Dokumentation an:

```bash
curl http://localhost:3000/api/rag/chat | python3 -m json.tool
```

**Gibt zurÃ¼ck:**
- Endpoint-Beschreibung
- Request/Response Format
- Beispiele
- Model-Info

---

## ğŸ§ª Test-Fragen (Beispiele)

### 1. UVP-Pflicht

```json
{
  "query": "Brauche ich eine UVP fÃ¼r ein Restaurant mit 80mÂ²?"
}
```

### 2. Dokumente

```json
{
  "query": "Welche Dokumente brauche ich fÃ¼r eine Gastro-Genehmigung?"
}
```

### 3. Betriebsbeschreibung

```json
{
  "query": "Was muss in der Betriebsbeschreibung stehen?"
}
```

### 4. Fristen

```json
{
  "query": "Wie lange dauert die Genehmigung?"
}
```

### 5. MA 36

```json
{
  "query": "Wer ist zustÃ¤ndig fÃ¼r die Genehmigung in Wien?"
}
```

### 6. Sicherheit

```json
{
  "query": "Welche Sicherheitsvorschriften gelten fÃ¼r Gastronomiebetriebe?"
}
```

### 7. LÃ¤rmschutz

```json
{
  "query": "Was muss ich bezÃ¼glich LÃ¤rmschutz beachten?",
  "userContext": {
    "businessType": "nightclub",
    "outdoorSeating": true
  }
}
```

---

## âš™ï¸ Wie die API funktioniert

### Interne Pipeline:

```
1. Request kommt rein
   â†“
2. Validation (query vorhanden, max 1000 chars)
   â†“
3. performRAGQuery() aufrufen
   â”œâ”€ Embedding generieren (OpenAI)
   â”œâ”€ Pinecone durchsuchen (Top 5)
   â”œâ”€ Filter nach minScore=0.7
   â””â”€ Claude Response generieren
   â†“
4. Response formatieren & zurÃ¼ckgeben
```

### Performance:

**Typische Antwortzeiten:**
- Embedding: ~200-500ms
- Pinecone Search: ~100-300ms
- Claude Generation: ~2000-4000ms
- **Total: ~3-5 Sekunden**

**Token Usage:**
- Input: ~2000-3000 tokens (Quellen + Query)
- Output: ~300-800 tokens (Antwort)
- **Kosten: ~$0.001-0.003 pro Query**

---

## ğŸ¨ NÃ¤chste Schritte

### 1. **Frontend Chat UI erstellen**

**Datei:** `app/components/RAG/ChatInterface.tsx`

```tsx
// Chat-Komponente mit:
- Nachrichten-Liste
- Input-Feld
- Quellen-Anzeige
- Loading States
- Error Handling
```

### 2. **Guided Dialog integrieren**

**Datei:** `app/components/RAG/GuidedDialog.tsx`

```tsx
// Wizard mit:
- Schritt 1: Betriebsart?
- Schritt 2: GrÃ¶ÃŸe?
- Schritt 3: Standort?
- â†’ Kontext an Chat-API Ã¼bergeben
```

### 3. **Analytics hinzufÃ¼gen**

Track:
- HÃ¤ufigste Fragen
- Durchschnittliche Response-Zeit
- User-Feedback (ğŸ‘/ğŸ‘)
- Quellen-Nutzung

### 4. **Chat History**

Speichere:
- User-Fragen
- Claude-Antworten
- Session-basiert oder User-basiert

---

## ğŸ”§ Troubleshooting

### Problem: "Query is required"

**LÃ¶sung:** Query-Parameter fehlt in Request Body

```json
{"query": "Deine Frage hier"}
```

### Problem: "Query too long"

**LÃ¶sung:** Query ist > 1000 Zeichen, kÃ¼rzer formulieren

### Problem: "Failed to connect"

**LÃ¶sung:** Server lÃ¤uft nicht

```bash
npm run dev
```

### Problem: "Embedding generation failed"

**LÃ¶sung:** OpenAI API Key prÃ¼fen

```bash
echo $OPENAI_API_KEY
# oder in .env.local prÃ¼fen
```

### Problem: "Vector search failed"

**LÃ¶sung:** Pinecone Connection prÃ¼fen

```bash
curl http://localhost:3000/api/rag/test
```

### Problem: "AI response generation failed"

**LÃ¶sung:** Anthropic API Key prÃ¼fen

```bash
echo $ANTHROPIC_API_KEY
# oder in .env.local prÃ¼fen
```

### Problem: "No relevant sources found"

**MÃ¶gliche Ursachen:**
- Frage zu spezifisch
- Keine relevanten Dokumente in Pinecone
- minScore zu hoch (default: 0.7)

**LÃ¶sung:** Versuche allgemeinere Frage oder prÃ¼fe ob Vektoren in Pinecone sind:

```bash
curl http://localhost:3000/api/rag/embed/status
```

---

## ğŸ“ Code-Referenzen

### Hauptdateien:

- **`app/api/rag/chat/route.ts`** - Chat API Route
- **`app/lib/ai/rag.ts`** - RAG Pipeline (`performRAGQuery`)
- **`app/lib/ai/openai.ts`** - Embedding Generation
- **`app/lib/vectordb/pinecone.ts`** - Vector Search
- **`app/lib/ai/anthropic.ts`** - Claude Response
- **`test-chat.sh`** - Test Script

### Konfiguration:

**`.env.local`:**
```bash
ANTHROPIC_API_KEY="..."
OPENAI_API_KEY="..."
PINECONE_API_KEY="..."
PINECONE_INDEX_NAME="gastro-genehmigung"

RAG_TOP_K="5"
RAG_MIN_SCORE="0.7"
```

---

## ğŸ¯ Was du jetzt hast

```
âœ… Funktionierende Chat-API
âœ… RAG Pipeline (Embedding â†’ Search â†’ Generation)
âœ… 601 Vektoren in Pinecone
âœ… Claude 3.5 Haiku Integration
âœ… User Context Support
âœ… Pinecone Filter Support
âœ… Error Handling
âœ… Performance Tracking
âœ… Test Script
âœ… VollstÃ¤ndige Dokumentation
```

---

## ğŸ’¡ Tipps

1. **User Context ist optional aber hilfreich:**
   - Claude kann bessere, spezifischere Antworten geben
   - Besonders fÃ¼r GrÃ¶ÃŸen-spezifische Fragen

2. **Pinecone Filter fÃ¼r gezielte Suche:**
   - Suche nur in bestimmten Dokumenten-Typen
   - z.B. nur Gewerbeordnung durchsuchen

3. **Monitoring:**
   - Schau dir die Logs an (Terminal mit `npm run dev`)
   - PrÃ¼fe Token Usage in Response
   - Track Response-Zeiten

4. **Kosten-Optimierung:**
   - ~$0.001-0.003 pro Query
   - Bei 1000 Queries: ~$1-3
   - Sehr gÃ¼nstig dank Haiku!

5. **QualitÃ¤t verbessern:**
   - Mehr/bessere PDFs hinzufÃ¼gen
   - Chunking-GrÃ¶ÃŸe anpassen (`CHUNK_SIZE` in .env.local)
   - minScore anpassen (`RAG_MIN_SCORE`)

---

## ğŸš€ FERTIG!

Dein **RAG-System ist komplett funktionsfÃ¤hig**:

```
âœ… PDFs extrahiert (Python)
âœ… 601 Vektoren in Pinecone
âœ… Chat-API funktioniert
âœ… Claude generiert Antworten
âœ… Quellen werden angezeigt
âœ… Alles dokumentiert
```

**Teste es jetzt:**

```bash
npm run dev
```

Dann in neuem Terminal:

```bash
./test-chat.sh
```

---

**ğŸ‰ Viel Erfolg mit deinem RAG-System! ğŸš€**
